# Inicia√ß√£o Cient√≠fica üë®‚Äçüéì

## **T√≠tulo do Projeto:** _Explorando Padr√µes do Com√©rcio Exterior da regi√£o de Itapira: Uma an√°lise Baseada em Dados._

### √Årea de Atua√ß√£o: Ci√™ncia de Dados

**Ao longo do Projeto estarei exercendo as seguintes fun√ß√µes:**
- Arquiteto de Dados: Respons√°vel por criar o design que ser√° utilizado pelo Engenheiro de Dados
- Engenheiro de Dados: Coletar e armazenar os dados da Web
- Analista de Dados: Analisar, prever e evitar perdas usando os dados, com o intuito de auxiliar na tomada de decis√£o

**Disciplinas Relacionadas ao Projeto:**
- T√©cnicas de Programa√ß√£o
- Banco de Dados
- Estat√≠stica
- Banco de Dados n√£o Relacional
- Minera√ß√£o de Dados

**Objetivos:**
O objetivo deste projeto √© realizar a extra√ß√£o, processamento, armazenamento, an√°lise e visualiza√ß√£o de Dados do com√©rcio exterior da regi√£o de Itapira, com o intuito de identificar padr√µes, tend√™ncias e insights relevantes para auxiliar na compreens√£o e tomadas de decis√µes estrat√©gicas no √¢mbito do com√©rcio internacional. Especificamente, busca-se:
- Coletar e extrair dados relacionados √†s importa√ß√µes e exporta√ß√µes da regi√£o de Itapira, considerando produtos, pa√≠ses de origem, valores, volumes e outras vari√°veis relevantes;
- Realizar uma an√°lise descritiva dos dados extra√≠dos, identificando os principais fluxos comerciais, setores de destaque, sazonalidade e varia√ß√µes ao longo do tempo;
- Aplicar t√©cnicas Estat√≠sticas e de minera√ß√£o de dados para identificar padr√µes, correla√ß√µes e insights adicionais nos dados do com√©rcio exterior;
- Desenvolver visualiza√ß√µes interativas e intuitivas dos dados extra√≠dos, utilizando ferramentas adequadas, a fim de facilitar a compreens√£o e a explora√ß√£o dos resultados.

# Etapas do Projeto üìÜ

## M√™s: Agosto ‚åõ
**Atividades Previstas:**
- Defini√ß√£o do Problema de pesquisa
- Revis√£o da literatura sobre com√©rcio exterior, an√°lise de dados e visualiza√ß√£o de informa√ß√µes.
- Identifica√ß√£o e acesso √†s fontes de dados relevantes do com√©rcio exterior de Itapira.

## M√™s: Setembro ‚åõ
**Atividades Previstas:**
- Estudo das ferramentas envolvidas no Projeto
- Coleta de dados do com√©rcio Exterior.
- Limpeza e pr√©-processamento dos dados coletados.
- An√°lise Explorat√≥ria inicial dos dados, identificando padr√µes e caracter√≠sticas relevantes.

## M√™s: Outubro ‚åõ
**Atividades Previstas:**
- An√°lise estat√≠stica dos dados.
- Desenvolvimento das visualiza√ß√µes iniciais dos dados do com√©rcio exterior.
- Interpreta√ß√£o preliminar dos resultados obtidos.

## M√™s: Novembro ‚åõ
**Atividades Previstas:**
- Refinamento das an√°lises estat√≠sticas e realiza√ß√£o de an√°lises mais profundas.
- Aperfei√ßoamento das visualiza√ß√µes de dados, buscando maior clareza e impacto visual.
- Discuss√£o dos resultados em rela√ß√£o √†s hip√≥teses ou objetivos propostos.

## M√™s: Dezembro ‚åõ
**Atividades Previstas:**
- An√°lise final dos resultados, destacando insights e conclus√µes principais.
- Elabora√ß√£o final do relat√≥rio do projeto, incluindo introdu√ß√£o, metodologia, resultados, discuss√£o e conclus√µes.
- Revis√£o e formata√ß√£o do relat√≥rio. Apresenta√ß√£o dos resultados e conclus√µes do projeto.

## Ferramentas Utilizadas üî®

- Google Docs
- Vscode
- Power BI
- SQL Server
- Pentaho
- GitHub
- ChatGPT
- Google Colab
- Docker
- Powerpoint
- WBS Chart Pro
- DBdiagram.io
- GanttProject

  ### Linguagem de Programa√ß√£o usada üë®‚Äçüíª
  - Python

  # Pipelines de Dados

A fun√ß√£o primordial de um engenheiro de dados √© construir pipelines eficientes para uma empresa. Neste contexto, entendemos que os pipelines s√£o respons√°veis por transportar dados de um local para outro, realizando os devidos tratamentos durante essa passagem. Portanto, as principais etapas que ser√£o abordadas durante este projeto s√£o as seguintes:

**Extra√ß√£o:**
- Nesta etapa, os dados brutos s√£o coletados de v√°rias fontes para serem armazenados na √°rea de prepara√ß√£o.

**Transforma√ß√£o:**
- Esta fase √© respons√°vel por processar os dados brutos, de modo que eles adquiram valor e contribuam para a tomada de decis√µes.

**Carregamento:**
- Os dados processados s√£o finalmente direcionados para o destino final, onde podem ser utilizados por um Analista de Dados.

<img src="https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/e7e4f090-140d-4f33-9854-4845cec6036d" >

  # Etapa1: Extra√ß√£o e download dos arquivos no formato CSV

## Extra√ß√£o de Dados do site: _Estat√≠sticas de Com√©rcio Exterior em Dados Abertos_

[Link do site](https://www.gov.br/produtividade-e-comercio-exterior/pt-br/assuntos/comercio-exterior/estatisticas/base-de-dados-bruta)

O objetivo primordial de um Engenharia de Dados √© tornar os dados utiliz√°veis, atrav√©s de um conjunto de t√©cnicas para assim poder ser feita toda a an√°lise de dados em cima para auxiliar na tomada de decis√£o. Logo, a extra√ß√£o de dados √© a primeira etapa para todo Engenheiro. Em virtude disso, eu tive que utilizar as seguintes Bibliotecas para Extra√ß√£o:

- **request:** Utilizada para realizar a requisi√ß√£o ao site para dessa forma poder capturar e gravar os dados.
- **Pandas:** Utilizada para as consultas e leituras de dados de forma din√¢mica.
- **io:** Utilizada para lidar com diversos tipos de entradas arquivos de texto, bin√°rio e dados brutos.
- **urllib3:** Essa biblioteca foi de fundamental utilidade no decorrer das atividades. Sempre que fazemos uma requisi√ß√£o ao servidor, algum tipo de erro pode acontecer no caminho impedindo assim nossa requisi√ß√£o. Nessa linha de pensamento, toda vez que eu fazia uma requisi√ß√£o ao servidor, eu era impedido de acessar o conte√∫do por motivos de SSL, que tem o intuito de criptografar os dados e permitir seguran√ßa dos dados. Ent√£o, ele me impedia de pegar esses dados por seguran√ßa. No entanto, utilizando a referida biblioteca, eu pude ignorar essa chamada de aten√ß√£o, conseguindo trazer os dados para o meu ambiente de desenvolvimento.

### Ilustra√ß√£o dos c√≥digos:

![Imagem 1](https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/160762cf-2862-43ed-aab1-e5fc9366f620)

![Imagem 2](https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/e0490ea3-7dcb-434e-a23f-dc4406ec6d00)

# Etapa 2: Elabora√ß√£o do Modelo Estrela ‚≠ê

Ap√≥s concluir a extra√ß√£o de todos os dados, surge a necessidade de armazen√°-los de maneira estruturada e segura. Isso √© fundamental para possibilitar an√°lises estat√≠sticas posteriormente. Diante desse cen√°rio, optou-se pela implementa√ß√£o do modelo de banco de dados estrela.

Esse modelo foi selecionado devido √† sua capacidade de agilizar as consultas SQL. A abordagem se destaca ao utilizar uma tabela principal para centralizar todos os relacionamentos. Essa estrutura √© especialmente vantajosa, uma vez que os demais relacionamentos n√£o possuem conex√µes diretas entre si, mas sim apenas com a tabela de fatos.

Dessa forma, o uso do modelo estrela oferece maior efici√™ncia na realiza√ß√£o das an√°lises estat√≠sticas, ao mesmo tempo que garante a integridade e seguran√ßa dos dados armazenados.
<img src = "https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/1e3bdb6d-df2a-4854-9205-485e1e083cc6">

# Etapa 3: Transforma√ß√£o dos Dados üîÑ

O processo de transforma√ß√£o √© uma etapa fundamental para um engenheiro de dados. Tal etapa consiste em padronizar um conjunto de dados seguindo uma regra de neg√≥cio espec√≠fica. No projeto apresentado, o tratamento dos arquivos CSV foi padronizado seguindo as regras envolvidas em um Banco de Dados Relacional. Nesse contexto, a ferramenta Pentaho foi de suma import√¢ncia para algumas etapas de transforma√ß√£o dos dados, juntamente com o Banco de Dados SQL Server. Portanto, a uni√£o dessas duas ferramentas garante a consist√™ncia e integridade dos dados ao longo deste projeto.

## Pentaho

A incorpora√ß√£o do Pentaho desempenhou um papel essencial na otimiza√ß√£o de certos processos de transforma√ß√£o que, de outra forma, exigiriam um extenso esfor√ßo manual no SQL Server. Assim, foi crucial estabelecer uma conex√£o efetiva entre essa ferramenta e o banco de dados alvo. Isso permitiu que, ao concluir a transforma√ß√£o, os dados fossem automaticamente integrados ao banco.

<img src = "https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/e18d72c8-5e0c-4c6f-b808-d7dab1d8c2ee" width="1000px">

## Transforma√ß√µes

No decorrer desta etapa, foram realizadas diversas transforma√ß√µes e limpezas nos dados, sendo as mais utilizadas:

1. Ordenamento dos valores presentes nos campos.
2. Remo√ß√£o de valores duplicados.
3. Cria√ß√£o de uma chave prim√°ria (PK).
4. Mudan√ßa da tipagem dos dados.
5. Cria√ß√£o das chaves estrangeiras (FK).
6. Sele√ß√£o das colunas desejadas.
7. Granulariza√ß√£o dos campos Dim_Data.
8. Adi√ß√£o de colunas.
9. Altera√ß√£o dos nomes das tabelas.

### Ordenamento dos valores presentes nos campos

Para conseguir remover os dados duplicados, antes de tudo, √© necess√°rio ordenar os valores presentes no campo.

### Remo√ß√£o de valores duplicados e cria√ß√£o de chaves prim√°rias

Como estamos normalizando os dados para o Padr√£o Relacional, torna-se necess√°rio um campo do tipo PK em nossa base de dados. Para garantir que um campo seja considerado PK, o mesmo tem algumas restri√ß√µes que devem ser seguidas para garantir a integridade dos relacionamentos. Uma dessas restri√ß√µes √© que a chave n√£o tenha valor duplicado, portanto, foi necess√°rio fazer a retirada de valores duplicados ao longo desta tabela. Em seguida, no SQL Server, alterar o tipo do campo para PK. A seguir, ser√£o ilustrados tais processos:

<img src="https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/45069be9-30e3-4b3a-b54d-615991b1e096" width="1000px">
<img src="https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/44056fc9-788f-44eb-9438-e22ef7ed46a2" width="1000px">
<img src="https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/92c01813-0bec-46ec-8d8e-19a4e6c0c023" width="1000px">


### Mudan√ßa da tipagem dos dados e cria√ß√£o de chaves estrangeiras

Ao criar uma chave estrangeira para assegurar relacionamentos com tabelas de dimens√µes, √© crucial que os dados sejam compat√≠veis a fim de garantir essa conex√£o. Em algumas situa√ß√µes, foi necess√°rio ajustar a tipagem dos dados nas tabelas de dimens√µes para assegurar essa correspond√™ncia.

<img src = "https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/5568e5c6-0b43-411e-b9f9-9782b8e87aec" width = "1000px">

### Sele√ß√£o das colunas desejadas

Para a cria√ß√£o da tabela Dim_Data, foi necess√°rio selecionar algumas colunas desejadas, como "CO_ANO" e "CO_MES", de um arquivo CSV que possui mais de 7 campos. Logo, foi necess√°rio excluir alguns campos e priorizar os dois campos mencionados utilizando a fun√ß√£o `SELECT VALUES` do Pentaho. Dessa forma, o resultado foi obtido com sucesso.

<img src = "https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/099207db-3b25-4f07-bf4b-0f7e3f701898" width = "1000px">

### Granulariza√ß√£o dos campos Dim_Data

O processo de granulariza√ß√£o envolve o aumento do n√≠vel de detalhamento de uma tabela, expandindo o n√∫mero de campos. Esse procedimento adquire extrema relev√¢ncia em cen√°rios de larga escala, onde auxilia as empresas na tomada de decis√µes fundamentais. Nesse contexto, os campos CO_ANO e CO_MES passaram por essa opera√ß√£o, conforme ilustrado na imagem abaixo:
 <img src= "https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/37a55bcd-0824-4dfb-9815-3fa5bbd10fb3" width="1000px">

### Adi√ß√£o de Coluna

Na constru√ß√£o da tabela fato, foi necess√°rio utilizar um identificador desse campo, j√° que o mesmo n√£o possu√≠a um campo que realizava determinada a√ß√£o antes de ser tratado. Com isso, o campo ID_EXPORTA√á√ïES foi criado.

<img src="https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/56a34820-4ad3-4145-94b2-543182a08e49" width = "1000px">

### Altera√ß√£o dos nomes das tabelas

Para seguir a padroniza√ß√£o do modelo Estrela, que consiste em ter tabelas com o sufixo "Dim" e "Fato", foi necess√°rio alterar o nome das tabelas extra√≠das do site do governo. Isso foi realizado pelo Pentaho, permitindo assim enviar tal altera√ß√£o para o SQL SERVER.

# Etapa 4: Carregamento de Dados üíæ

Para garantir a organiza√ß√£o e o armazenamento eficiente dos dados, √© imperativo que eles tenham um ambiente dedicado no SQL Server. Para atender a essa necessidade, √© estabelecido um DataWarehouse, que funciona como um reposit√≥rio centralizado capaz de armazenar e gerenciar os dados previamente existentes no SQL Server. Isso n√£o apenas assegura a integridade dos dados, mas tamb√©m fornece uma estrutura adequada para an√°lises e consultas estrat√©gicas.

<img src = "https://github.com/julianoAlessandro/AnaliseEstatisticaPython/assets/111141842/015d375a-71ff-4069-88d0-610cd7a83df5" width = "1000px">

